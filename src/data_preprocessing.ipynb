{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:40:50.291425Z",
     "start_time": "2025-12-04T07:40:35.158571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "OUTPUT_DIR = r\"F:\\Ai&ml\\outputs\"\n",
    "DATASET_DIR = os.path.join(OUTPUT_DIR, \"datasets\")\n",
    "VIZ_DIR = os.path.join(OUTPUT_DIR, \"visualization\")\n",
    "\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(VIZ_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LIVER CIRRHOSIS DATA PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput directories created:\")\n",
    "print(f\"  Datasets: {DATASET_DIR}\")\n",
    "print(f\"  Visualizations: {VIZ_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.read_csv('../data/raw/liver_cirrhosis.csv')\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "with open(os.path.join(DATASET_DIR, 'data_summary.txt'), 'w') as f:\n",
    "    f.write(\"LIVER CIRRHOSIS DATASET SUMMARY\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Shape: {df.shape}\\n\")\n",
    "    f.write(f\"Rows: {df.shape[0]}\\n\")\n",
    "    f.write(f\"Columns: {df.shape[1]}\\n\\n\")\n",
    "    f.write(\"Column Names:\\n\")\n",
    "    f.write(str(list(df.columns)) + \"\\n\\n\")\n",
    "    f.write(\"Data Types:\\n\")\n",
    "    f.write(str(df.dtypes) + \"\\n\\n\")\n",
    "    f.write(\"First 5 Rows:\\n\")\n",
    "    f.write(str(df.head()) + \"\\n\\n\")\n",
    "    f.write(\"Descriptive Statistics:\\n\")\n",
    "    f.write(str(df.describe(include='all')) + \"\\n\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: INITIAL DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if 'Stage' in numeric_cols:\n",
    "    numeric_cols.remove('Stage')\n",
    "\n",
    "print(f\"\\nNumeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "missing_values = df.isna().sum()\n",
    "missing_percent = (missing_values / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percent': missing_percent.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "missing_df.to_csv(os.path.join(DATASET_DIR, 'missing_values_report.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nGenerating target distribution plot...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "stage_counts = df['Stage'].value_counts().sort_index()\n",
    "ax = sns.countplot(x='Stage', data=df, palette='viridis')\n",
    "plt.title('Distribution of Cirrhosis Stages', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Stage', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "for i, v in enumerate(stage_counts.values):\n",
    "    ax.text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, '01_stage_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: 01_stage_distribution.png\")\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"\\nGenerating missing values heatmap...\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df.isna(), cbar=True, cmap='YlOrRd', yticklabels=False)\n",
    "    plt.title('Missing Values Heatmap', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Samples', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, '02_missing_values_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  ✓ Saved: 02_missing_values_heatmap.png\")\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nGenerating correlation matrix...\")\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
    "                center=0, fmt='.2f', square=True, linewidths=1)\n",
    "    plt.title('Numeric Features Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, '03_correlation_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  ✓ Saved: 03_correlation_matrix.png\")\n",
    "\n",
    "    corr_matrix.to_csv(os.path.join(DATASET_DIR, 'correlation_matrix.csv'))\n",
    "\n",
    "print(\"\\nGenerating numeric feature distributions...\")\n",
    "n_numeric = len(numeric_cols)\n",
    "if n_numeric > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (n_numeric + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if n_numeric > 1 else [axes]\n",
    "\n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        sns.histplot(df[col].dropna(), kde=True, ax=axes[idx], color='skyblue')\n",
    "        axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "    for idx in range(n_numeric, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, '04_numeric_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  ✓ Saved: 04_numeric_distributions.png\")\n",
    "\n",
    "print(\"\\nGenerating box plots by stage...\")\n",
    "n_numeric = len(numeric_cols)\n",
    "if n_numeric > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (n_numeric + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if n_numeric > 1 else [axes]\n",
    "\n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        sns.boxplot(x='Stage', y=col, data=df, ax=axes[idx], palette='Set2')\n",
    "        axes[idx].set_title(f'{col} by Stage', fontweight='bold')\n",
    "        axes[idx].set_xlabel('Stage')\n",
    "        axes[idx].set_ylabel(col)\n",
    "\n",
    "    for idx in range(n_numeric, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, '05_boxplots_by_stage.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  ✓ Saved: 05_boxplots_by_stage.png\")\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nGenerating categorical feature distributions...\")\n",
    "    cat_cols_no_target = [col for col in categorical_cols if col != 'Stage']\n",
    "\n",
    "    for idx, col in enumerate(cat_cols_no_target):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.countplot(x=col, hue='Stage', data=df, palette='muted')\n",
    "        plt.title(f'Distribution of {col} by Stage', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(col, fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Stage', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIZ_DIR, f'06_{idx+1}_categorical_{col}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✓ Saved: 06_{idx+1}_categorical_{col}.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X = df.drop('Stage', axis=1)\n",
    "y = df['Stage']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTrain set target distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nTest set target distribution:\")\n",
    "print(y_test.value_counts().sort_index())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n[1/4] Imputing missing values...\")\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_train_copy = X_train.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    X_train_copy[numeric_cols] = numeric_imputer.fit_transform(X_train_copy[numeric_cols])\n",
    "    X_test_copy[numeric_cols] = numeric_imputer.transform(X_test_copy[numeric_cols])\n",
    "\n",
    "cat_cols_no_target = [col for col in categorical_cols if col != 'Stage']\n",
    "if len(cat_cols_no_target) > 0:\n",
    "    X_train_copy[cat_cols_no_target] = categorical_imputer.fit_transform(X_train_copy[cat_cols_no_target])\n",
    "    X_test_copy[cat_cols_no_target] = categorical_imputer.transform(X_test_copy[cat_cols_no_target])\n",
    "\n",
    "print(f\"  ✓ Train missing values: {X_train_copy.isna().sum().sum()}\")\n",
    "print(f\"  ✓ Test missing values: {X_test_copy.isna().sum().sum()}\")\n",
    "\n",
    "print(\"\\n[2/4] Encoding target variable...\")\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "target_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(f\"  ✓ Target encoding: {target_mapping}\")\n",
    "\n",
    "print(\"\\n[3/4] One-hot encoding categorical variables...\")\n",
    "if len(cat_cols_no_target) > 0:\n",
    "    X_train_encoded = pd.get_dummies(X_train_copy, columns=cat_cols_no_target, drop_first=True)\n",
    "    X_test_encoded = pd.get_dummies(X_test_copy, columns=cat_cols_no_target, drop_first=True)\n",
    "\n",
    "    X_train_encoded, X_test_encoded = X_train_encoded.align(\n",
    "        X_test_encoded, join='left', axis=1, fill_value=0\n",
    "    )\n",
    "    print(f\"  ✓ Features after encoding: {X_train_encoded.shape[1]}\")\n",
    "else:\n",
    "    X_train_encoded = X_train_copy.copy()\n",
    "    X_test_encoded = X_test_copy.copy()\n",
    "\n",
    "print(\"\\n[4/4] Scaling numeric features...\")\n",
    "scaler = StandardScaler()\n",
    "if len(numeric_cols) > 0:\n",
    "    X_train_encoded[numeric_cols] = scaler.fit_transform(X_train_encoded[numeric_cols])\n",
    "    X_test_encoded[numeric_cols] = scaler.transform(X_test_encoded[numeric_cols])\n",
    "    print(f\"  ✓ Scaled {len(numeric_cols)} numeric features\")\n",
    "    print(f\"  ✓ Sample feature means: {X_train_encoded[numeric_cols].mean().round(3).to_dict()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: SAVING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "joblib.dump(X_train_encoded, os.path.join(DATASET_DIR, 'X_train.joblib'))\n",
    "joblib.dump(X_test_encoded, os.path.join(DATASET_DIR, 'X_test.joblib'))\n",
    "joblib.dump(y_train_encoded, os.path.join(DATASET_DIR, 'y_train.joblib'))\n",
    "joblib.dump(y_test_encoded, os.path.join(DATASET_DIR, 'y_test.joblib'))\n",
    "print(\"  ✓ Saved: X_train.joblib, X_test.joblib, y_train.joblib, y_test.joblib\")\n",
    "\n",
    "X_train_encoded.to_csv(os.path.join(DATASET_DIR, 'X_train.csv'), index=False)\n",
    "X_test_encoded.to_csv(os.path.join(DATASET_DIR, 'X_test.csv'), index=False)\n",
    "pd.DataFrame(y_train_encoded, columns=['Stage']).to_csv(\n",
    "    os.path.join(DATASET_DIR, 'y_train.csv'), index=False\n",
    ")\n",
    "pd.DataFrame(y_test_encoded, columns=['Stage']).to_csv(\n",
    "    os.path.join(DATASET_DIR, 'y_test.csv'), index=False\n",
    ")\n",
    "print(\"  ✓ Saved: CSV versions of all datasets\")\n",
    "\n",
    "joblib.dump(le, os.path.join(DATASET_DIR, 'label_encoder.joblib'))\n",
    "joblib.dump(scaler, os.path.join(DATASET_DIR, 'scaler.joblib'))\n",
    "joblib.dump(numeric_imputer, os.path.join(DATASET_DIR, 'numeric_imputer.joblib'))\n",
    "joblib.dump(categorical_imputer, os.path.join(DATASET_DIR, 'categorical_imputer.joblib'))\n",
    "joblib.dump(list(X_train_encoded.columns), os.path.join(DATASET_DIR, 'feature_names.joblib'))\n",
    "print(\"  ✓ Saved: All preprocessing objects\")\n",
    "\n",
    "summary = {\n",
    "    'original_shape': df.shape,\n",
    "    'n_features_original': X.shape[1],\n",
    "    'n_features_processed': X_train_encoded.shape[1],\n",
    "    'n_train_samples': len(X_train_encoded),\n",
    "    'n_test_samples': len(X_test_encoded),\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'categorical_cols': cat_cols_no_target,\n",
    "    'target_classes': list(le.classes_),\n",
    "    'target_mapping': target_mapping,\n",
    "    'train_class_distribution': dict(pd.Series(y_train_encoded).value_counts().sort_index()),\n",
    "    'test_class_distribution': dict(pd.Series(y_test_encoded).value_counts().sort_index()),\n",
    "    'feature_names': list(X_train_encoded.columns)\n",
    "}\n",
    "\n",
    "joblib.dump(summary, os.path.join(DATASET_DIR, 'preprocessing_summary.joblib'))\n",
    "\n",
    "with open(os.path.join(DATASET_DIR, 'preprocessing_summary.txt'), 'w') as f:\n",
    "    f.write(\"PREPROCESSING SUMMARY\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    for key, value in summary.items():\n",
    "        f.write(f\"{key}:\\n{value}\\n\\n\")\n",
    "\n",
    "print(\"  ✓ Saved: preprocessing_summary.joblib and .txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n DATASET SUMMARY:\")\n",
    "print(f\"  • Original samples: {df.shape[0]}\")\n",
    "print(f\"  • Original features: {df.shape[1]}\")\n",
    "print(f\"  • Processed features: {X_train_encoded.shape[1]}\")\n",
    "print(f\"  • Train samples: {len(X_train_encoded)}\")\n",
    "print(f\"  • Test samples: {len(X_test_encoded)}\")\n",
    "\n",
    "print(f\"\\n OUTPUT LOCATIONS:\")\n",
    "print(f\"  • Datasets: {DATASET_DIR}\")\n",
    "print(f\"  • Visualizations: {VIZ_DIR}\")\n",
    "\n",
    "print(f\"\\n FILES CREATED:\")\n",
    "print(f\"  • {len(os.listdir(DATASET_DIR))} dataset files\")\n",
    "print(f\"  • {len(os.listdir(VIZ_DIR))} visualization files\")\n",
    "\n",
    "print(\"\\nAll preprocessing steps completed!\")\n",
    "print(\"All files saved successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"By_OwenXAGK\")"
   ],
   "id": "5176d0da3a8727de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LIVER CIRRHOSIS DATA PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "Output directories created:\n",
      "  Datasets: F:\\Ai&ml\\outputs\\datasets\n",
      "  Visualizations: F:\\Ai&ml\\outputs\\visualization\n",
      "\n",
      "============================================================\n",
      "STEP 1: LOADING DATA\n",
      "============================================================\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (25000, 19)\n",
      "Columns: ['N_Days', 'Status', 'Drug', 'Age', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']\n",
      "\n",
      "First 5 rows:\n",
      "   N_Days Status     Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n",
      "0    2221      C  Placebo  18499   F       N            Y       N     N   \n",
      "1    1230      C  Placebo  19724   M       Y            N       Y     N   \n",
      "2    4184      C  Placebo  11839   F       N            N       N     N   \n",
      "3    2090      D  Placebo  16467   F       N            N       N     N   \n",
      "4    2105      D  Placebo  21699   F       N            Y       N     N   \n",
      "\n",
      "   Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n",
      "0        0.5        149.0     4.04   227.0     598.0   52.70           57.0   \n",
      "1        0.5        219.0     3.93    22.0     663.0   45.00           75.0   \n",
      "2        0.5        320.0     3.54    51.0    1243.0  122.45           80.0   \n",
      "3        0.7        255.0     3.74    23.0    1024.0   77.50           58.0   \n",
      "4        1.9        486.0     3.54    74.0    1052.0  108.50          109.0   \n",
      "\n",
      "   Platelets  Prothrombin  Stage  \n",
      "0      256.0          9.9      1  \n",
      "1      220.0         10.8      2  \n",
      "2      225.0         10.0      2  \n",
      "3      151.0         10.2      2  \n",
      "4      151.0         11.5      1  \n",
      "\n",
      "============================================================\n",
      "STEP 2: INITIAL DATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Numeric columns (11): ['N_Days', 'Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin']\n",
      "Categorical columns (7): ['Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
      "\n",
      "Missing Values Summary:\n",
      "No missing values found!\n",
      "\n",
      "============================================================\n",
      "STEP 3: EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Generating target distribution plot...\n",
      "  ✓ Saved: 01_stage_distribution.png\n",
      "\n",
      "Generating correlation matrix...\n",
      "  ✓ Saved: 03_correlation_matrix.png\n",
      "\n",
      "Generating numeric feature distributions...\n",
      "  ✓ Saved: 04_numeric_distributions.png\n",
      "\n",
      "Generating box plots by stage...\n",
      "  ✓ Saved: 05_boxplots_by_stage.png\n",
      "\n",
      "Generating categorical feature distributions...\n",
      "  ✓ Saved: 06_1_categorical_Status.png\n",
      "  ✓ Saved: 06_2_categorical_Drug.png\n",
      "  ✓ Saved: 06_3_categorical_Sex.png\n",
      "  ✓ Saved: 06_4_categorical_Ascites.png\n",
      "  ✓ Saved: 06_5_categorical_Hepatomegaly.png\n",
      "  ✓ Saved: 06_6_categorical_Spiders.png\n",
      "  ✓ Saved: 06_7_categorical_Edema.png\n",
      "\n",
      "============================================================\n",
      "STEP 4: TRAIN-TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "Train set size: 20000 (80.0%)\n",
      "Test set size: 5000 (20.0%)\n",
      "\n",
      "Train set target distribution:\n",
      "Stage\n",
      "1    6612\n",
      "2    6753\n",
      "3    6635\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set target distribution:\n",
      "Stage\n",
      "1    1653\n",
      "2    1688\n",
      "3    1659\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "STEP 5: DATA PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "[1/4] Imputing missing values...\n",
      "  ✓ Train missing values: 0\n",
      "  ✓ Test missing values: 0\n",
      "\n",
      "[2/4] Encoding target variable...\n",
      "  ✓ Target encoding: {np.int64(1): np.int64(0), np.int64(2): np.int64(1), np.int64(3): np.int64(2)}\n",
      "\n",
      "[3/4] One-hot encoding categorical variables...\n",
      "  ✓ Features after encoding: 20\n",
      "\n",
      "[4/4] Scaling numeric features...\n",
      "  ✓ Scaled 11 numeric features\n",
      "  ✓ Sample feature means: {'N_Days': 0.0, 'Age': -0.0, 'Bilirubin': -0.0, 'Cholesterol': -0.0, 'Albumin': -0.0, 'Copper': -0.0, 'Alk_Phos': -0.0, 'SGOT': 0.0, 'Tryglicerides': 0.0, 'Platelets': -0.0, 'Prothrombin': 0.0}\n",
      "\n",
      "============================================================\n",
      "STEP 6: SAVING PROCESSED DATA\n",
      "============================================================\n",
      "  ✓ Saved: X_train.joblib, X_test.joblib, y_train.joblib, y_test.joblib\n",
      "  ✓ Saved: CSV versions of all datasets\n",
      "  ✓ Saved: All preprocessing objects\n",
      "  ✓ Saved: preprocessing_summary.joblib and .txt\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      " DATASET SUMMARY:\n",
      "  • Original samples: 25000\n",
      "  • Original features: 19\n",
      "  • Processed features: 20\n",
      "  • Train samples: 20000\n",
      "  • Test samples: 5000\n",
      "\n",
      " OUTPUT LOCATIONS:\n",
      "  • Datasets: F:\\Ai&ml\\outputs\\datasets\n",
      "  • Visualizations: F:\\Ai&ml\\outputs\\visualization\n",
      "\n",
      " FILES CREATED:\n",
      "  • 18 dataset files\n",
      "  • 33 visualization files\n",
      "\n",
      "All preprocessing steps completed!\n",
      "All files saved successfully!\n",
      "\n",
      "============================================================\n",
      "By_OwenXAGK\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

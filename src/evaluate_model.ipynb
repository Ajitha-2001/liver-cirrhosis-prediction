{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:50:22.532962Z",
     "start_time": "2025-12-04T07:50:13.925228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    f1_score, accuracy_score, precision_score, recall_score\n",
    ")\n",
    "import shap\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "OUTPUT_DIR = r\"F:\\Ai&ml\\outputs\"\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, \"results\")\n",
    "VIZ_DIR = os.path.join(OUTPUT_DIR, \"visualization\")\n",
    "DATASET_DIR = os.path.join(OUTPUT_DIR, \"datasets\")\n",
    "SHAP_DIR = os.path.join(VIZ_DIR, \"shap_analysis\")\n",
    "\n",
    "os.makedirs(SHAP_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LIVER CIRRHOSIS MODEL EVALUATION & EXPLAINABILITY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput directories:\")\n",
    "print(f\"  ‚Ä¢ Models: {MODEL_DIR}\")\n",
    "print(f\"  ‚Ä¢ Results: {RESULTS_DIR}\")\n",
    "print(f\"  ‚Ä¢ Visualizations: {VIZ_DIR}\")\n",
    "print(f\"  ‚Ä¢ SHAP Analysis: {SHAP_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATA AND MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_test = joblib.load(os.path.join(DATASET_DIR, 'X_test.joblib'))\n",
    "y_test = joblib.load(os.path.join(DATASET_DIR, 'y_test.joblib'))\n",
    "feature_names = joblib.load(os.path.join(DATASET_DIR, 'feature_names.joblib'))\n",
    "\n",
    "label_encoder = joblib.load(os.path.join(DATASET_DIR, 'label_encoder.joblib'))\n",
    "class_names = [str(cls) for cls in label_encoder.classes_]\n",
    "\n",
    "try:\n",
    "    initial_model = joblib.load(os.path.join(MODEL_DIR, 'best_initial_model.joblib'))\n",
    "    initial_model_name = joblib.load(os.path.join(MODEL_DIR, 'best_model_name.joblib'))\n",
    "    print(f\"\\n‚úì Initial model loaded: {initial_model_name}\")\n",
    "except:\n",
    "    initial_model = None\n",
    "    print(\"\\n‚ö†Ô∏è  Initial model not found\")\n",
    "\n",
    "try:\n",
    "    tuned_model = joblib.load(os.path.join(MODEL_DIR, 'best_tuned_model.joblib'))\n",
    "    print(\"‚úì Tuned model loaded\")\n",
    "except:\n",
    "    tuned_model = None\n",
    "    print(\"‚ö†Ô∏è  Tuned model not found\")\n",
    "\n",
    "print(f\"\\n‚úì Test data loaded:\")\n",
    "print(f\"  ‚Ä¢ Samples: {len(X_test)}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(feature_names)}\")\n",
    "print(f\"  ‚Ä¢ Classes: {class_names}\")\n",
    "\n",
    "def evaluate_model_comprehensive(model, X_test, y_test, model_name,\n",
    "                                 class_names, save_prefix=\"model\"):\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    weighted_recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"\\nüìä Overall Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Weighted Precision: {weighted_precision:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Weighted Recall: {weighted_recall:.4f}\")\n",
    "\n",
    "    report = classification_report(y_test, y_pred,\n",
    "                                   target_names=class_names,\n",
    "                                   output_dict=True,\n",
    "                                   zero_division=0)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    print(f\"\\nüìã Classification Report:\")\n",
    "    print(report_df.round(4))\n",
    "\n",
    "    report_df.to_csv(\n",
    "        os.path.join(RESULTS_DIR, f'{save_prefix}_classification_report.csv')\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                square=True, linewidths=1, linecolor='black',\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Confusion Matrix - {model_name}',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, f'{save_prefix}_confusion_matrix.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"\\n‚úì Confusion matrix saved\")\n",
    "\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                square=True, linewidths=1, linecolor='black',\n",
    "                cbar_kws={'label': 'Percentage'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Normalized Confusion Matrix - {model_name}',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, f'{save_prefix}_confusion_matrix_normalized.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Normalized confusion matrix saved\")\n",
    "\n",
    "    per_class_metrics = {\n",
    "        'Class': class_names,\n",
    "        'Precision': [report[cls]['precision'] for cls in class_names],\n",
    "        'Recall': [report[cls]['recall'] for cls in class_names],\n",
    "        'F1-Score': [report[cls]['f1-score'] for cls in class_names],\n",
    "        'Support': [report[cls]['support'] for cls in class_names]\n",
    "    }\n",
    "\n",
    "    metrics_df = pd.DataFrame(per_class_metrics)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for idx, metric in enumerate(['Precision', 'Recall', 'F1-Score']):\n",
    "        ax = axes[idx]\n",
    "        bars = ax.bar(metrics_df['Class'], metrics_df[metric],\n",
    "                     color='skyblue', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        ax.set_xlabel('Class', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{metric} by Class', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "    plt.suptitle(f'Per-Class Metrics - {model_name}',\n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, f'{save_prefix}_per_class_metrics.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Per-class metrics visualization saved\")\n",
    "\n",
    "    evaluation_results = {\n",
    "        'model_name': model_name,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'overall_metrics': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'weighted_f1': float(weighted_f1),\n",
    "            'macro_f1': float(macro_f1),\n",
    "            'weighted_precision': float(weighted_precision),\n",
    "            'weighted_recall': float(weighted_recall)\n",
    "        },\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'confusion_matrix_normalized': cm_normalized.tolist(),\n",
    "        'classification_report': report,\n",
    "        'per_class_metrics': metrics_df.to_dict('records')\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, f'{save_prefix}_evaluation_results.json'), 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "    print(f\"‚úì Evaluation results saved to JSON\")\n",
    "\n",
    "    return y_pred, report_df, evaluation_results\n",
    "\n",
    "def explain_model_with_shap(model, X_test, feature_names, model_name,\n",
    "                           sample_size=100, save_prefix=\"model\"):\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SHAP ANALYSIS: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        actual_model = model.named_steps['model']\n",
    "    else:\n",
    "        actual_model = model\n",
    "\n",
    "    X_sample = X_test.iloc[:sample_size] if hasattr(X_test, 'iloc') else X_test[:sample_size]\n",
    "\n",
    "    print(f\"\\n‚è≥ Computing SHAP values for {sample_size} samples...\")\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(actual_model)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"‚úì Multi-class SHAP values computed ({len(shap_values)} classes)\")\n",
    "        else:\n",
    "            print(f\"‚úì SHAP values computed\")\n",
    "\n",
    "        print(\"\\nüìä Generating SHAP summary plots...\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_sample,\n",
    "                         feature_names=feature_names,\n",
    "                         plot_type=\"bar\", show=False)\n",
    "        plt.title(f'SHAP Feature Importance - {model_name}',\n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SHAP_DIR, f'{save_prefix}_shap_importance.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"  ‚úì Feature importance plot saved\")\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_sample,\n",
    "                         feature_names=feature_names, show=False)\n",
    "        plt.title(f'SHAP Feature Effects - {model_name}',\n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SHAP_DIR, f'{save_prefix}_shap_summary.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"  ‚úì Feature effects plot saved\")\n",
    "\n",
    "        if isinstance(shap_values, list):\n",
    "            mean_shap = np.abs(np.array(shap_values)).mean(axis=0).mean(axis=0).flatten()\n",
    "        else:\n",
    "            mean_shap = np.abs(shap_values).mean(axis=0).flatten()\n",
    "\n",
    "        if isinstance(feature_names, np.ndarray):\n",
    "            feature_names_flat = feature_names.flatten().tolist()\n",
    "        elif isinstance(feature_names, pd.Index):\n",
    "            feature_names_flat = feature_names.tolist()\n",
    "        else:\n",
    "            feature_names_flat = list(feature_names)\n",
    "\n",
    "        if len(mean_shap) != len(feature_names_flat):\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: SHAP values length ({len(mean_shap)}) doesn't match feature names ({len(feature_names_flat)})\")\n",
    "            min_len = min(len(mean_shap), len(feature_names_flat))\n",
    "            mean_shap = mean_shap[:min_len]\n",
    "            feature_names_flat = feature_names_flat[:min_len]\n",
    "\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names_flat,\n",
    "            'mean_abs_shap': mean_shap\n",
    "        }).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "        feature_importance_df.to_csv(\n",
    "            os.path.join(SHAP_DIR, f'{save_prefix}_shap_feature_importance.csv'),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüèÜ Top 10 Most Important Features (by SHAP):\")\n",
    "        print(feature_importance_df.head(10).to_string(index=False))\n",
    "\n",
    "        print(\"\\nüìà Generating dependence plots for top 5 features...\")\n",
    "        top_features = feature_importance_df.head(5)['feature'].tolist()\n",
    "\n",
    "        for i, feature in enumerate(top_features):\n",
    "            try:\n",
    "                feature_idx = feature_names.index(feature)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "\n",
    "                if isinstance(shap_values, list):\n",
    "                    shap.dependence_plot(feature_idx, shap_values[0], X_sample,\n",
    "                                       feature_names=feature_names, show=False)\n",
    "                else:\n",
    "                    shap.dependence_plot(feature_idx, shap_values, X_sample,\n",
    "                                       feature_names=feature_names, show=False)\n",
    "\n",
    "                plt.title(f'SHAP Dependence Plot - {feature}',\n",
    "                         fontsize=14, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(SHAP_DIR,\n",
    "                           f'{save_prefix}_dependence_{i+1}_{feature}.png'),\n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"  ‚úì Dependence plot saved for {feature}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Could not create dependence plot for {feature}: {str(e)}\")\n",
    "\n",
    "        print(\"\\nüîç Generating force plots for sample predictions...\")\n",
    "        for i in range(min(5, len(X_sample))):\n",
    "            try:\n",
    "                plt.figure(figsize=(20, 3))\n",
    "\n",
    "                if isinstance(shap_values, list):\n",
    "                    class_idx = np.argmax([sv[i].sum() for sv in shap_values])\n",
    "                    shap.force_plot(explainer.expected_value[class_idx],\n",
    "                                  shap_values[class_idx][i],\n",
    "                                  X_sample.iloc[i] if hasattr(X_sample, 'iloc') else X_sample[i],\n",
    "                                  feature_names=feature_names,\n",
    "                                  matplotlib=True, show=False)\n",
    "                else:\n",
    "                    shap.force_plot(explainer.expected_value,\n",
    "                                  shap_values[i],\n",
    "                                  X_sample.iloc[i] if hasattr(X_sample, 'iloc') else X_sample[i],\n",
    "                                  feature_names=feature_names,\n",
    "                                  matplotlib=True, show=False)\n",
    "\n",
    "                plt.title(f'SHAP Force Plot - Sample {i+1}',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(SHAP_DIR,\n",
    "                           f'{save_prefix}_force_plot_sample_{i+1}.png'),\n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Could not create force plot for sample {i+1}: {str(e)}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ SHAP analysis completed successfully!\")\n",
    "\n",
    "        return shap_values, feature_importance_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during SHAP analysis: {str(e)}\")\n",
    "        print(\"This might occur if the model type is not supported by TreeExplainer.\")\n",
    "        return None, None\n",
    "\n",
    "def compare_models(results_dict):\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': list(results_dict.keys()),\n",
    "        'Accuracy': [r['overall_metrics']['accuracy'] for r in results_dict.values()],\n",
    "        'Weighted F1': [r['overall_metrics']['weighted_f1'] for r in results_dict.values()],\n",
    "        'Macro F1': [r['overall_metrics']['macro_f1'] for r in results_dict.values()],\n",
    "        'Precision': [r['overall_metrics']['weighted_precision'] for r in results_dict.values()],\n",
    "        'Recall': [r['overall_metrics']['weighted_recall'] for r in results_dict.values()]\n",
    "    })\n",
    "\n",
    "    print(\"\\nüìä Model Comparison:\")\n",
    "    print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "    comparison_df.to_csv(\n",
    "        os.path.join(RESULTS_DIR, 'model_comparison_detailed.csv'),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    metrics = ['Accuracy', 'Weighted F1', 'Macro F1', 'Precision', 'Recall']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(comparison_df))\n",
    "    width = 0.15\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax.bar(x + i*width, comparison_df[metric], width,\n",
    "               label=metric, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(x + width * 2)\n",
    "    ax.set_xticklabels(comparison_df['Model'])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, 'model_comparison_all_metrics.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\n‚úì Comparison visualization saved\")\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    results_dict = {}\n",
    "\n",
    "    if initial_model is not None:\n",
    "        y_pred_initial, report_initial, results_initial = evaluate_model_comprehensive(\n",
    "            initial_model, X_test, y_test,\n",
    "            initial_model_name, class_names,\n",
    "            save_prefix=\"initial_model\"\n",
    "        )\n",
    "        results_dict[f'{initial_model_name} (Initial)'] = results_initial\n",
    "\n",
    "        shap_values_initial, shap_importance_initial = explain_model_with_shap(\n",
    "            initial_model, X_test, feature_names,\n",
    "            initial_model_name,\n",
    "            sample_size=100,\n",
    "            save_prefix=\"initial_model\"\n",
    "        )\n",
    "\n",
    "    if tuned_model is not None:\n",
    "        y_pred_tuned, report_tuned, results_tuned = evaluate_model_comprehensive(\n",
    "            tuned_model, X_test, y_test,\n",
    "            \"Tuned Model\", class_names,\n",
    "            save_prefix=\"tuned_model\"\n",
    "        )\n",
    "        results_dict['Tuned Model'] = results_tuned\n",
    "\n",
    "        shap_values_tuned, shap_importance_tuned = explain_model_with_shap(\n",
    "            tuned_model, X_test, feature_names,\n",
    "            \"Tuned Model\",\n",
    "            sample_size=100,\n",
    "            save_prefix=\"tuned_model\"\n",
    "        )\n",
    "\n",
    "    if len(results_dict) > 1:\n",
    "        comparison_df = compare_models(results_dict)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ EVALUATION & EXPLAINABILITY COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚è±Ô∏è  Total execution time: {duration:.2f} seconds\")\n",
    "    print(f\"\\nüìÅ All outputs saved to:\")\n",
    "    print(f\"  ‚Ä¢ Results: {RESULTS_DIR}\")\n",
    "    print(f\"  ‚Ä¢ Visualizations: {VIZ_DIR}\")\n",
    "    print(f\"  ‚Ä¢ SHAP Analysis: {SHAP_DIR}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"By_OwenXAGK\")"
   ],
   "id": "b992b6dfdd8e8c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LIVER CIRRHOSIS MODEL EVALUATION & EXPLAINABILITY\n",
      "======================================================================\n",
      "\n",
      "Output directories:\n",
      "  ‚Ä¢ Models: F:\\Ai&ml\\outputs\\models\n",
      "  ‚Ä¢ Results: F:\\Ai&ml\\outputs\\results\n",
      "  ‚Ä¢ Visualizations: F:\\Ai&ml\\outputs\\visualization\n",
      "  ‚Ä¢ SHAP Analysis: F:\\Ai&ml\\outputs\\visualization\\shap_analysis\n",
      "\n",
      "======================================================================\n",
      "LOADING DATA AND MODELS\n",
      "======================================================================\n",
      "\n",
      "‚úì Initial model loaded: XGBoost\n",
      "‚úì Tuned model loaded\n",
      "\n",
      "‚úì Test data loaded:\n",
      "  ‚Ä¢ Samples: 5000\n",
      "  ‚Ä¢ Features: 20\n",
      "  ‚Ä¢ Classes: ['1', '2', '3']\n",
      "\n",
      "======================================================================\n",
      "EVALUATING: XGBoost\n",
      "======================================================================\n",
      "\n",
      "üìä Overall Metrics:\n",
      "  ‚Ä¢ Accuracy: 0.9598\n",
      "  ‚Ä¢ Weighted F1: 0.9598\n",
      "  ‚Ä¢ Macro F1: 0.9599\n",
      "  ‚Ä¢ Weighted Precision: 0.9600\n",
      "  ‚Ä¢ Weighted Recall: 0.9598\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision  recall  f1-score    support\n",
      "1                0.9637  0.9468    0.9551  1653.0000\n",
      "2                0.9431  0.9627    0.9528  1688.0000\n",
      "3                0.9734  0.9699    0.9716  1659.0000\n",
      "accuracy         0.9598  0.9598    0.9598     0.9598\n",
      "macro avg        0.9601  0.9598    0.9599  5000.0000\n",
      "weighted avg     0.9600  0.9598    0.9598  5000.0000\n",
      "\n",
      "‚úì Confusion matrix saved\n",
      "‚úì Normalized confusion matrix saved\n",
      "‚úì Per-class metrics visualization saved\n",
      "‚úì Evaluation results saved to JSON\n",
      "\n",
      "======================================================================\n",
      "SHAP ANALYSIS: XGBoost\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Computing SHAP values for 100 samples...\n",
      "‚úì SHAP values computed\n",
      "\n",
      "üìä Generating SHAP summary plots...\n",
      "  ‚úì Feature importance plot saved\n",
      "  ‚úì Feature effects plot saved\n",
      "  ‚ö†Ô∏è  Warning: SHAP values length (60) doesn't match feature names (20)\n",
      "\n",
      "üèÜ Top 10 Most Important Features (by SHAP):\n",
      "      feature  mean_abs_shap\n",
      "        Sex_M       0.565054\n",
      "    Bilirubin       0.530008\n",
      "     Status_D       0.447826\n",
      " Drug_Placebo       0.384784\n",
      "       Copper       0.374112\n",
      "       N_Days       0.355956\n",
      "      Albumin       0.351998\n",
      "Tryglicerides       0.335236\n",
      "  Cholesterol       0.314336\n",
      "          Age       0.249048\n",
      "\n",
      "üìà Generating dependence plots for top 5 features...\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Sex_M: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Bilirubin: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Status_D: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Drug_Placebo: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Copper: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "\n",
      "üîç Generating force plots for sample predictions...\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 1: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 2: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 3: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 4: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 5: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "\n",
      "‚úÖ SHAP analysis completed successfully!\n",
      "\n",
      "======================================================================\n",
      "EVALUATING: Tuned Model\n",
      "======================================================================\n",
      "\n",
      "üìä Overall Metrics:\n",
      "  ‚Ä¢ Accuracy: 0.9624\n",
      "  ‚Ä¢ Weighted F1: 0.9624\n",
      "  ‚Ä¢ Macro F1: 0.9624\n",
      "  ‚Ä¢ Weighted Precision: 0.9625\n",
      "  ‚Ä¢ Weighted Recall: 0.9624\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision  recall  f1-score    support\n",
      "1                0.9668  0.9516    0.9591  1653.0000\n",
      "2                0.9481  0.9639    0.9559  1688.0000\n",
      "3                0.9728  0.9717    0.9723  1659.0000\n",
      "accuracy         0.9624  0.9624    0.9624     0.9624\n",
      "macro avg        0.9626  0.9624    0.9624  5000.0000\n",
      "weighted avg     0.9625  0.9624    0.9624  5000.0000\n",
      "\n",
      "‚úì Confusion matrix saved\n",
      "‚úì Normalized confusion matrix saved\n",
      "‚úì Per-class metrics visualization saved\n",
      "‚úì Evaluation results saved to JSON\n",
      "\n",
      "======================================================================\n",
      "SHAP ANALYSIS: Tuned Model\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Computing SHAP values for 100 samples...\n",
      "‚úì SHAP values computed\n",
      "\n",
      "üìä Generating SHAP summary plots...\n",
      "  ‚úì Feature importance plot saved\n",
      "  ‚úì Feature effects plot saved\n",
      "  ‚ö†Ô∏è  Warning: SHAP values length (60) doesn't match feature names (20)\n",
      "\n",
      "üèÜ Top 10 Most Important Features (by SHAP):\n",
      "      feature  mean_abs_shap\n",
      "        Sex_M       0.554950\n",
      "    Bilirubin       0.516104\n",
      " Drug_Placebo       0.411662\n",
      "     Status_D       0.391522\n",
      "       N_Days       0.389505\n",
      "       Copper       0.347554\n",
      "      Albumin       0.341224\n",
      "  Cholesterol       0.309546\n",
      "Tryglicerides       0.302432\n",
      "          Age       0.263043\n",
      "\n",
      "üìà Generating dependence plots for top 5 features...\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Sex_M: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Bilirubin: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Drug_Placebo: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for Status_D: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "  ‚ö†Ô∏è  Could not create dependence plot for N_Days: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 10\n",
      "\n",
      "üîç Generating force plots for sample predictions...\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 1: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 2: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 3: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 4: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "  ‚ö†Ô∏è  Could not create force plot for sample 5: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "\n",
      "‚úÖ SHAP analysis completed successfully!\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üìä Model Comparison:\n",
      "            Model  Accuracy  Weighted F1  Macro F1  Precision  Recall\n",
      "XGBoost (Initial)    0.9598       0.9598    0.9599     0.9600  0.9598\n",
      "      Tuned Model    0.9624       0.9624    0.9624     0.9625  0.9624\n",
      "\n",
      "‚úì Comparison visualization saved\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EVALUATION & EXPLAINABILITY COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Total execution time: 8.04 seconds\n",
      "\n",
      "üìÅ All outputs saved to:\n",
      "  ‚Ä¢ Results: F:\\Ai&ml\\outputs\\results\n",
      "  ‚Ä¢ Visualizations: F:\\Ai&ml\\outputs\\visualization\n",
      "  ‚Ä¢ SHAP Analysis: F:\\Ai&ml\\outputs\\visualization\\shap_analysis\n",
      "\n",
      "======================================================================\n",
      "By_OwenXAGK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

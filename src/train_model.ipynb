{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:49:13.493409Z",
     "start_time": "2025-12-04T07:46:51.010050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, make_scorer\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "OUTPUT_DIR = r\"F:\\Ai&ml\\outputs\"\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, \"results\")\n",
    "VIZ_DIR = os.path.join(OUTPUT_DIR, \"visualization\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(VIZ_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LIVER CIRRHOSIS MODEL TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput directories:\")\n",
    "print(f\"  â€¢ Models: {MODEL_DIR}\")\n",
    "print(f\"  â€¢ Results: {RESULTS_DIR}\")\n",
    "print(f\"  â€¢ Visualizations: {VIZ_DIR}\")\n",
    "\n",
    "def weighted_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'weighted_f1': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'macro_f1': f1_score(y_test, y_pred, average='macro'),\n",
    "        'weighted_precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'weighted_recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    }\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return metrics, report, cm, y_pred\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name, save_path):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "                square=True, linewidths=1, linecolor='black')\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_comparison(results_dict, metric='weighted_f1'):\n",
    "    models = list(results_dict.keys())\n",
    "    scores = [results_dict[m]['cv_mean'] for m in models]\n",
    "    std = [results_dict[m]['cv_std'] for m in models]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(models, scores, yerr=std, capsize=5,\n",
    "                   color='skyblue', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.title(f'Model Comparison - Cross-Validation {metric.replace(\"_\", \" \").title()}',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, f'model_comparison_{metric}.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name, top_n=20):\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        estimator = model.named_steps['model']\n",
    "    else:\n",
    "        estimator = model\n",
    "\n",
    "    if hasattr(estimator, 'feature_importances_'):\n",
    "        importances = estimator.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:top_n]\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(top_n), importances[indices], color='teal')\n",
    "        plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Importance', fontsize=12)\n",
    "        plt.title(f'Top {top_n} Feature Importances - {model_name}',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIZ_DIR, f'feature_importance_{model_name.replace(\" \", \"_\")}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': [feature_names[i] for i in indices],\n",
    "            'importance': importances[indices]\n",
    "        })\n",
    "        importance_df.to_csv(\n",
    "            os.path.join(RESULTS_DIR, f'feature_importance_{model_name.replace(\" \", \"_\")}.csv'),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "def train_initial_models(X_train, y_train, X_test, y_test, feature_names):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 1: TRAINING INITIAL MODELS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=100, random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=100, random_state=42,\n",
    "            eval_metric='mlogloss', n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=100, random_state=42,\n",
    "            n_jobs=-1, verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=100, random_state=42,\n",
    "            verbose=0, thread_count=-1\n",
    "        )\n",
    "    }\n",
    "\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(f\"\\nTraining set class distribution:\")\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {cnt} samples ({cnt/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "    use_smote = len(unique) > 1 and min(counts) < 50\n",
    "    if use_smote:\n",
    "        print(\"\\nâš ï¸  Detected class imbalance. Applying SMOTE...\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "    else:\n",
    "        print(\"\\nâœ“ Balanced dataset. Skipping SMOTE...\")\n",
    "        smote = None\n",
    "\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n[{list(models.keys()).index(name) + 1}/{len(models)}] Training {name}...\")\n",
    "\n",
    "        if use_smote:\n",
    "            pipeline = ImbPipeline(steps=[\n",
    "                ('smote', smote),\n",
    "                ('model', model)\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = ImbPipeline(steps=[('model', model)])\n",
    "\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, X_train, y_train,\n",
    "            cv=cv_strategy,\n",
    "            scoring=make_scorer(weighted_f1),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        metrics, report, cm, y_pred = evaluate_model(\n",
    "            pipeline, X_test, y_test, name\n",
    "        )\n",
    "\n",
    "        results[name] = {\n",
    "            'cv_mean': np.mean(cv_scores),\n",
    "            'cv_std': np.std(cv_scores),\n",
    "            'cv_scores': cv_scores.tolist(),\n",
    "            'test_metrics': metrics,\n",
    "            'classification_report': report\n",
    "        }\n",
    "\n",
    "        trained_models[name] = pipeline\n",
    "\n",
    "        print(f\"  âœ“ CV F1 Score: {np.mean(cv_scores):.4f} (Â±{np.std(cv_scores):.4f})\")\n",
    "        print(f\"  âœ“ Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  âœ“ Test F1 Score: {metrics['weighted_f1']:.4f}\")\n",
    "\n",
    "        plot_confusion_matrix(\n",
    "            cm, name,\n",
    "            os.path.join(VIZ_DIR, f'confusion_matrix_{name.replace(\" \", \"_\")}.png')\n",
    "        )\n",
    "\n",
    "        plot_feature_importance(pipeline, feature_names, name)\n",
    "\n",
    "    best_model_name = max(results, key=lambda x: results[x]['cv_mean'])\n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_score = results[best_model_name]['cv_mean']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ† BEST MODEL: {best_model_name}\")\n",
    "    print(f\"   CV F1 Score: {best_score:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    joblib.dump(best_model, os.path.join(MODEL_DIR, 'best_initial_model.joblib'))\n",
    "    joblib.dump(best_model_name, os.path.join(MODEL_DIR, 'best_model_name.joblib'))\n",
    "\n",
    "    for name, model in trained_models.items():\n",
    "        safe_name = name.replace(' ', '_').lower()\n",
    "        joblib.dump(model, os.path.join(MODEL_DIR, f'{safe_name}_model.joblib'))\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, 'initial_model_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'CV_Mean': [results[m]['cv_mean'] for m in results],\n",
    "        'CV_Std': [results[m]['cv_std'] for m in results],\n",
    "        'Test_Accuracy': [results[m]['test_metrics']['accuracy'] for m in results],\n",
    "        'Test_F1': [results[m]['test_metrics']['weighted_f1'] for m in results]\n",
    "    }).sort_values('CV_Mean', ascending=False)\n",
    "\n",
    "    summary_df.to_csv(os.path.join(RESULTS_DIR, 'model_comparison_summary.csv'), index=False)\n",
    "\n",
    "    plot_model_comparison(results)\n",
    "\n",
    "    return best_model, best_model_name, results, trained_models\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, X_test, y_test,\n",
    "                         feature_names, base_model_name=None):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: HYPERPARAMETER TUNING\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    param_grids = {\n",
    "        'XGBoost': {\n",
    "            'model__n_estimators': [100, 200, 300, 500],\n",
    "            'model__max_depth': [3, 5, 7, 9],\n",
    "            'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'model__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'model__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'model__gamma': [0, 0.1, 0.2, 0.3],\n",
    "            'model__min_child_weight': [1, 3, 5]\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'model__n_estimators': [100, 200, 300, 500],\n",
    "            'model__max_depth': [3, 5, 7, 9, -1],\n",
    "            'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'model__num_leaves': [15, 31, 63, 127],\n",
    "            'model__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'model__colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model__n_estimators': [100, 200, 300, 500],\n",
    "            'model__max_depth': [10, 20, 30, None],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4],\n",
    "            'model__max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__max_depth': [3, 5, 7],\n",
    "            'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'model__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'model__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'CatBoost': {\n",
    "            'model__iterations': [100, 200, 300, 500],\n",
    "            'model__depth': [4, 6, 8, 10],\n",
    "            'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'model__l2_leaf_reg': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if base_model_name and base_model_name in param_grids:\n",
    "        model_name = base_model_name\n",
    "    else:\n",
    "        model_name = 'XGBoost'\n",
    "\n",
    "    print(f\"\\nğŸ” Tuning {model_name}...\")\n",
    "\n",
    "    if model_name == 'XGBoost':\n",
    "        base_model = XGBClassifier(random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "    elif model_name == 'LightGBM':\n",
    "        base_model = LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
    "    elif model_name == 'Random Forest':\n",
    "        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        base_model = GradientBoostingClassifier(random_state=42)\n",
    "    elif model_name == 'CatBoost':\n",
    "        base_model = CatBoostClassifier(random_state=42, verbose=0, thread_count=-1)\n",
    "\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    use_smote = len(unique) > 1 and min(counts) < 50\n",
    "\n",
    "    if use_smote:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        pipeline = ImbPipeline(steps=[('smote', smote), ('model', base_model)])\n",
    "    else:\n",
    "        pipeline = ImbPipeline(steps=[('model', base_model)])\n",
    "\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=30,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring=make_scorer(weighted_f1),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâ³ Running RandomizedSearchCV with {random_search.n_iter} iterations...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_cv_score = random_search.best_score_\n",
    "\n",
    "    print(f\"\\nâœ… Best CV F1 Score: {best_cv_score:.4f}\")\n",
    "    print(f\"\\nğŸ“‹ Best Parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  â€¢ {param}: {value}\")\n",
    "\n",
    "    metrics, report, cm, y_pred = evaluate_model(\n",
    "        best_model, X_test, y_test, f\"{model_name} (Tuned)\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ“Š Test Set Performance:\")\n",
    "    print(f\"  â€¢ Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  â€¢ Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
    "    print(f\"  â€¢ Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "\n",
    "    joblib.dump(best_model, os.path.join(MODEL_DIR, 'best_tuned_model.joblib'))\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, 'best_hyperparameters.json'), 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    tuning_results = {\n",
    "        'model_name': model_name,\n",
    "        'best_cv_score': best_cv_score,\n",
    "        'best_params': best_params,\n",
    "        'test_metrics': metrics,\n",
    "        'cv_results': {\n",
    "            'mean_test_score': random_search.cv_results_['mean_test_score'].tolist(),\n",
    "            'std_test_score': random_search.cv_results_['std_test_score'].tolist(),\n",
    "            'params': [str(p) for p in random_search.cv_results_['params']]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, 'tuning_results.json'), 'w') as f:\n",
    "        json.dump(tuning_results, f, indent=4)\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        cm, f\"{model_name} (Tuned)\",\n",
    "        os.path.join(VIZ_DIR, f'confusion_matrix_tuned_{model_name.replace(\" \", \"_\")}.png')\n",
    "    )\n",
    "\n",
    "    plot_feature_importance(best_model, feature_names, f\"{model_name} (Tuned)\")\n",
    "\n",
    "    cv_results_df = pd.DataFrame({\n",
    "        'iteration': range(len(random_search.cv_results_['mean_test_score'])),\n",
    "        'mean_score': random_search.cv_results_['mean_test_score'],\n",
    "        'std_score': random_search.cv_results_['std_test_score']\n",
    "    }).sort_values('mean_score', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.errorbar(cv_results_df['iteration'], cv_results_df['mean_score'],\n",
    "                yerr=cv_results_df['std_score'], fmt='o-', capsize=3, alpha=0.7)\n",
    "    plt.axhline(y=best_cv_score, color='r', linestyle='--',\n",
    "                label=f'Best Score: {best_cv_score:.4f}')\n",
    "    plt.xlabel('Iteration', fontsize=12)\n",
    "    plt.ylabel('CV F1 Score', fontsize=12)\n",
    "    plt.title(f'Hyperparameter Tuning Results - {model_name}',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, 'hyperparameter_tuning_results.png'),\n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return best_model, best_params, tuning_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    DATA_DIR = r\"F:\\Ai&ml\\outputs\\datasets\"\n",
    "    X_train = joblib.load(os.path.join(DATA_DIR, 'X_train.joblib'))\n",
    "    y_train = joblib.load(os.path.join(DATA_DIR, 'y_train.joblib'))\n",
    "    X_test = joblib.load(os.path.join(DATA_DIR, 'X_test.joblib'))\n",
    "    y_test = joblib.load(os.path.join(DATA_DIR, 'y_test.joblib'))\n",
    "    feature_names = joblib.load(os.path.join(DATA_DIR, 'feature_names.joblib'))\n",
    "\n",
    "    print(f\"\\nâœ“ Data loaded successfully!\")\n",
    "    print(f\"  â€¢ Training samples: {len(X_train)}\")\n",
    "    print(f\"  â€¢ Test samples: {len(X_test)}\")\n",
    "    print(f\"  â€¢ Features: {len(feature_names)}\")\n",
    "\n",
    "    best_initial_model, best_model_name, initial_results, all_models = train_initial_models(\n",
    "        X_train, y_train, X_test, y_test, feature_names\n",
    "    )\n",
    "\n",
    "    best_tuned_model, best_params, tuning_results = hyperparameter_tuning(\n",
    "        X_train, y_train, X_test, y_test, feature_names, best_model_name\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL MODEL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    initial_score = initial_results[best_model_name]['test_metrics']['weighted_f1']\n",
    "    tuned_score = tuning_results['test_metrics']['weighted_f1']\n",
    "    improvement = ((tuned_score - initial_score) / initial_score) * 100\n",
    "\n",
    "    print(f\"\\n{best_model_name}:\")\n",
    "    print(f\"  â€¢ Initial Test F1: {initial_score:.4f}\")\n",
    "    print(f\"  â€¢ Tuned Test F1: {tuned_score:.4f}\")\n",
    "    print(f\"  â€¢ Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "    final_report = {\n",
    "        'execution_time_seconds': duration,\n",
    "        'best_initial_model': best_model_name,\n",
    "        'initial_best_score': initial_score,\n",
    "        'tuned_best_score': tuned_score,\n",
    "        'improvement_percent': improvement,\n",
    "        'timestamp': end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, 'final_training_report.json'), 'w') as f:\n",
    "        json.dump(final_report, f, indent=4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… TRAINING PIPELINE COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nâ±ï¸  Total execution time: {duration:.2f} seconds\")\n",
    "    print(f\"\\nğŸ“ All outputs saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"  â€¢ Models: {MODEL_DIR}\")\n",
    "    print(f\"  â€¢ Results: {RESULTS_DIR}\")\n",
    "    print(f\"  â€¢ Visualizations: {VIZ_DIR}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"By_OwenXAGK\")"
   ],
   "id": "d6e4a52be13e120c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LIVER CIRRHOSIS MODEL TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Output directories:\n",
      "  â€¢ Models: F:\\Ai&ml\\outputs\\models\n",
      "  â€¢ Results: F:\\Ai&ml\\outputs\\results\n",
      "  â€¢ Visualizations: F:\\Ai&ml\\outputs\\visualization\n",
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "\n",
      "âœ“ Data loaded successfully!\n",
      "  â€¢ Training samples: 20000\n",
      "  â€¢ Test samples: 5000\n",
      "  â€¢ Features: 20\n",
      "\n",
      "======================================================================\n",
      "STEP 1: TRAINING INITIAL MODELS\n",
      "======================================================================\n",
      "\n",
      "Training set class distribution:\n",
      "  Class 0: 6612 samples (33.1%)\n",
      "  Class 1: 6753 samples (33.8%)\n",
      "  Class 2: 6635 samples (33.2%)\n",
      "\n",
      "âœ“ Balanced dataset. Skipping SMOTE...\n",
      "\n",
      "[1/5] Training Random Forest...\n",
      "  âœ“ CV F1 Score: 0.9495 (Â±0.0027)\n",
      "  âœ“ Test Accuracy: 0.9520\n",
      "  âœ“ Test F1 Score: 0.9520\n",
      "\n",
      "[2/5] Training Gradient Boosting...\n",
      "  âœ“ CV F1 Score: 0.8528 (Â±0.0047)\n",
      "  âœ“ Test Accuracy: 0.8426\n",
      "  âœ“ Test F1 Score: 0.8429\n",
      "\n",
      "[3/5] Training XGBoost...\n",
      "  âœ“ CV F1 Score: 0.9594 (Â±0.0005)\n",
      "  âœ“ Test Accuracy: 0.9598\n",
      "  âœ“ Test F1 Score: 0.9598\n",
      "\n",
      "[4/5] Training LightGBM...\n",
      "  âœ“ CV F1 Score: 0.9565 (Â±0.0013)\n",
      "  âœ“ Test Accuracy: 0.9558\n",
      "  âœ“ Test F1 Score: 0.9558\n",
      "\n",
      "[5/5] Training CatBoost...\n",
      "  âœ“ CV F1 Score: 0.9415 (Â±0.0040)\n",
      "  âœ“ Test Accuracy: 0.9348\n",
      "  âœ“ Test F1 Score: 0.9348\n",
      "\n",
      "======================================================================\n",
      "ğŸ† BEST MODEL: XGBoost\n",
      "   CV F1 Score: 0.9594\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 2: HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "\n",
      "ğŸ” Tuning XGBoost...\n",
      "\n",
      "â³ Running RandomizedSearchCV with 30 iterations...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "âœ… Best CV F1 Score: 0.9635\n",
      "\n",
      "ğŸ“‹ Best Parameters:\n",
      "  â€¢ model__subsample: 1.0\n",
      "  â€¢ model__n_estimators: 300\n",
      "  â€¢ model__min_child_weight: 5\n",
      "  â€¢ model__max_depth: 7\n",
      "  â€¢ model__learning_rate: 0.2\n",
      "  â€¢ model__gamma: 0.1\n",
      "  â€¢ model__colsample_bytree: 0.8\n",
      "\n",
      "ğŸ“Š Test Set Performance:\n",
      "  â€¢ Accuracy: 0.9624\n",
      "  â€¢ Weighted F1: 0.9624\n",
      "  â€¢ Macro F1: 0.9624\n",
      "\n",
      "======================================================================\n",
      "FINAL MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "XGBoost:\n",
      "  â€¢ Initial Test F1: 0.9598\n",
      "  â€¢ Tuned Test F1: 0.9624\n",
      "  â€¢ Improvement: +0.27%\n",
      "\n",
      "======================================================================\n",
      "âœ… TRAINING PIPELINE COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  Total execution time: 142.43 seconds\n",
      "\n",
      "ğŸ“ All outputs saved to: F:\\Ai&ml\\outputs\n",
      "  â€¢ Models: F:\\Ai&ml\\outputs\\models\n",
      "  â€¢ Results: F:\\Ai&ml\\outputs\\results\n",
      "  â€¢ Visualizations: F:\\Ai&ml\\outputs\\visualization\n",
      "\n",
      "======================================================================\n",
      "By_OwenXAGK\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
